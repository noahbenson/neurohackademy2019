{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retinotopic Maps and Model-based Analysis of fMRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Noah C. Benson &lt;[nben@nyu.edu](mailto:nben@nyu.edu)&gt;  \n",
    "**Repository**: [https://github.com/noahbenson/neurohackademy2019](https://github.com/noahbenson/neurohackademy2019)\n",
    "\n",
    "This notebook is an introduction to retinotopic maps and model-based analysis of fMRI, created for the Neurohackademy 2019 summer course by Noah C. Benson. In this notebook, we will use Python and in particular the [neuropythy](https://github.com/noahbenson/neuropythy) and [popeye](https://github.com/kdesimone/popeye) open source libraries to download both anatomical/structural data and functional data (BOLD time-series) from the [Human Commectome Project](https://db.humanconnectome.org/) then to analyze the time-series. This notebook is intended to be a verbose description of the various tools and techniques involved in such an analysis; it is likely that we will not cover all the material in the notebook during the Neurohackademy lecture.\n",
    "\n",
    "The contents of this notebook are as follows:\n",
    "\n",
    "* **Introduction**  \n",
    "  This section gives a brief introduction to the project in the notebook.\n",
    "* **Initialization**  \n",
    "  In this section of the notebook, we import libraries, configure the plotting environment, and declare custom functions that we will need throughout the notebook. We will also verify that you have configured your environment correctly for this notebook to work.\n",
    "* **Data Wrangling**  \n",
    "  Here, we will load the relevant anatomical data from the HCP's Amazon S3 interface using [neuropythy](https://github.com/noahbenson/neuropythy) then plot verify those data by plotting them. We will then load and plot function fMRI data (i.e., BOLD time-series) from the HCP.\n",
    "* **Population Receptive Field Modeling**  \n",
    "  This section documents population receptive field (pRF) models and demonstrates how to fit a pRF model from the fMRI data obtained in the previous section.\n",
    "* **Retinotopic Maps**  \n",
    "  Finally, we examine retinotopic maps--the organization of pRFs across the cortical surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import a number of libraries and configure matplotlib/pyplot, our 2D plotting tool. We then check that neuropythy is able to communicate with the Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Configure Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some standard/utility libraries:\n",
    "import os, sys, time, h5py\n",
    "import six           # six provides python 2/3 compatibility\n",
    "\n",
    "# Import our numerical/scientific libraries, scipy and numpy:\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# The pimms (Python Immutables) library is a utility library that enables lazy\n",
    "# computation and immutble data structures; https://github.com/noahbenson/pimms\n",
    "import pimms\n",
    "\n",
    "# The popeye library (https://github.com/kdesimone/popeye) is a pRF modeling\n",
    "# library that we will use to obtain pRF parameters for the functional data\n",
    "import popeye\n",
    "\n",
    "# The neuropythy library is a swiss-army-knife for handling MRI data, especially\n",
    "# anatomical/structural data such as that produced by FreeSurfer or the HCP.\n",
    "# https://github.com/noahbenson/neuropythy\n",
    "import neuropythy as ny\n",
    "\n",
    "# Import graphics libraries:\n",
    "# Matplotlib/Pyplot is our 2D graphing library:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# We also use the 3D graphics library ipyvolume for 3D surface rendering\n",
    "import ipyvolume as ipv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These \"magic commands\" tell matplotlib that we want to plot figures inline and\n",
    "# That we are using qt as a backend; due to bugs in certain versions of\n",
    "# matplotlib, we put them in a separate cell as the import statements above and\n",
    "# the configuration statements below.\n",
    "%gui qt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional matplotlib preferences:\n",
    "font_data = {'family':'sans-serif',\n",
    "             'sans-serif':['Helvetica Neue', 'Helvetica', 'Arial'],\n",
    "             'size': 10,\n",
    "             'weight': 'light'}\n",
    "mpl.rc('font',**font_data)\n",
    "# we want relatively high-res images, especially when saving to disk.\n",
    "mpl.rcParams['figure.dpi'] = 72*2\n",
    "mpl.rcParams['savefig.dpi'] = 72*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check neuropythy's Amazon S3 Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [neuropythy](https://github.com/noahbenson/neuropythy) library can easily be configured to automatically download HCP data it is requested. In order to do this, however, it must have been given a set of HCP credentials. The HCP uses the Amazon S3 so these credentials are in the form of a \"key\" and a \"secret\". To obtain HCP credentials, you must register at the [HCP database website](https://db.humanconnectome.org/) then generate Amazon S3 credentials through their interface. The [neuropythy configuration documentation](https://github.com/noahbenson/neuropythy/wiki/Configuration) explains how to do this in more detail.\n",
    "\n",
    "Your credentials will look something like \"`AKAIG8RT71SWARPYUFUS`\" and \"`TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4`\" (key and secret). They are often printed with a \"`:`\" between them like \"`AKAIG8RT71SWARPYUFUS:TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4`\". If, when you started up the docker-container running this notebook, you provided your credentials as a `:`-separated string like this via the environment variable HCP_CREDENTIALS, you should be fine. If neuropythy found your credentials, they should be in neuropythy's `config` structure, which behaves like a Python dictionary. If neuropythy could not read your credentials, then this value will be `None`. If this is the case, you can either:\n",
    " * set the credentials directly in this notebook by running something like:  \n",
    "   ```python\n",
    "   key = 'AKAIG8RT71SWARPYUFUS'\n",
    "   secret = 'TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4'\n",
    "   ny.config['hcp_credentials'] = (key, secret)\n",
    "   ```\n",
    " * restart the docker container after configuring the `HCP_CREDENTIALS` environment variable:\n",
    "   ```bash\n",
    "   > export HCP_CREDENTIALS=\"AKAIG8RT71SWARPYUFUS:TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4\"\n",
    "   > docker-compose up\n",
    "   ```\n",
    " * store the credentials in a local file and import its contents into the `HCP_CREDENTIALS` environment variable:\n",
    "   ```bash\n",
    "   > echo \"AKAIG8RT71SWARPYUFUS:TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4\" > ~/.hcp-passwd\n",
    "   > export HCP_CREDENTIALS=\"`cat ~/.hcp-passwd`\"\n",
    "   > docker-compose up\n",
    "   ```\n",
    "\n",
    "We also want to check that neuropythy was able to connect to the HCP database. If this fails, it is possible that either your credentials are incorrect/expired or that you do not have a valid internet connection, or that you did something unexpected when mounting volumes into the docker that prevent neuropythy from knowing where to store the HCP data (unlikely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that HCP credentials were found:\n",
    "if ny.config['hcp_credentials'] is None:\n",
    "    raise Exception('No valid HCP credentials were found!\\n'\n",
    "                    'See above instructions for remedy.')\n",
    "\n",
    "# Check that we can access the HCP database:\n",
    "# To do this we grab the 's3fs' object from neuropythy's 'hcp' dataset; this\n",
    "# object maintains a connection to Amazon's S3 using the hcp credentials. We use\n",
    "# it to perform a basic ls operation on the S3 filesystem. If this fails, we do\n",
    "# not have a working connection to the S3.\n",
    "try: files = ny.data['hcp'].s3fs.ls('hcp-openaccess')\n",
    "except Exception: files = None\n",
    "if files is None:\n",
    "    raise Exception('Could not communicate with S3!\\n'\n",
    "                    'This probably indicates that your credentials are wrong'\n",
    "                    ' or that you do not have an internet connection.')\n",
    "\n",
    "print('Configuration appears fine!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many projects, the task of wrangling data into a format that enables your analysis is 98% of the work, but becomes only 2% of the resulting scientific paper. When dealing with fMRI data, this difficulty often arises from the complex and often poorly-documented formats and directory structures used to store the data. Both FreeSurfer and the HCP Pipelines are capable of processing a T1-weighted anatomical image into a rich set of (mostly equivalent) information about the cortical surface of the brain, its anatomy, and how its geometry maps onto anatomical atlases (like FreeSurfer's *fsaverage* subject or the HCP's *fs_LR* hemispheres). However, interpreting these data is a chore, and, even if you understand the directories and formats, the amount of code required to load the relevant data for an analysis can be quite substantial.\n",
    "\n",
    "One of the main goals of neuropythy is to reduce the burden of both understanding and coding access to these data. For the human connectome project in particular, neuropythy will automatically download and manage structural data as if it were all always on your local storage without you needing to think about it. This section will demo these abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a sample subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the following demos, we need to choose an example HCP subject to use. I have chosen one below, but you should be able to use basically any HCP subject you like, so long as they are in the HCP_1200 release. To see a list of these subjects, you can ask neuropythy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 10 HCP_1200 subjects:\n",
    "ny.hcp.subject_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many HCP_1200 subjects are there?\n",
    "len(ny.hcp.subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to be doing analysis on the retinotopic maps, we will eventually want to choose a subject that was part of the retinotopic mapping experiments. To get a list of these subjects, we can ask neuropythy's 'hcp_retinotopy' dataset for its list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many retinotopy subjects?\n",
    "len(ny.data['hcp_retinotopy'].subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are actually only 181 HCP subjects with retinotopy, but the dataset includes three additional subjects: subject 999999 (group average of all 181 subjects) and subjects 999998 and 999997 (group average of split halves of the 181 subjects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the subject we will use for the following demos\n",
    "sid = 100610\n",
    "\n",
    "# Make sure that's a real subject\n",
    "if sid not in ny.hcp.subject_ids:\n",
    "    raise Exception('Subject %s is not an HCP_1200 subject!' % (sid,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1: Load a Subject and Plot their Pial Surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will load the HCP subject we picked above using neuropythy; neuropythy will give us a subject object that encapsulates a large amount of data about the subject, including volumetric MR images as well as surface-based data. We will focus mostly on the surface data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the subject object:\n",
    "sub = ny.hcp_subject(sid)\n",
    "\n",
    "# We want to plot both the LH and the RH pial surfaces; this code uses the 'lh'\n",
    "# and 'rh' hemispheres which are the subject's \"native\" hemispheres. The HCP\n",
    "# also includes a number of \"atlas\" hemispheres that are roughly equivalent but\n",
    "# that have been coerced to a common geometrical tesselation. These hemispheres\n",
    "# contain less accurate representations of the subject's structural data but\n",
    "# have the advantage that vertices on the surface are approximately aligned\n",
    "# between subjects. For a list of all supported hemispheres, including atlas\n",
    "# hemispheres, you can see sub.hemis, which is a dictionary of all the known\n",
    "# hemisphere objects. sub.lh and sub.rh are aliases for sub.hemis['lh'] and\n",
    "# sub.hemis['rh'].\n",
    "\n",
    "surf = 'pial'       # we're plotting the pial surface\n",
    "view = 'posterior'  # we want a view of the posterior of the brain\n",
    "\n",
    "# make the plot; because we are passing in 3D surfaces, it returns an \n",
    "# ipyvolume figure (which we will not actually use)\n",
    "ny.cortex_plot((sub.lh.surfaces[surf], sub.rh.surfaces[surf]), view=view)\n",
    "\n",
    "# tell ipyvolume to show the plot:\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: Plot Structural Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometric structural data, such as the meshes plotted in the section above, are only one kind of data that are included in FreeSurfer and HCP subject directories. Neuropythy is capable of loading most of these data, and automatically organizes them, when possible, into properties that are associated with the hemisphere objects and surface meshes. An example of this is the cortical thickness (in mm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cortical thickness is actually just a vector of values, one per vertex in\n",
    "# the hemisphere's mesh representation. Properties are stored in numpy arrays\n",
    "# and can be accessed easily:\n",
    "lh_thickness = sub.lh.prop('thickness')\n",
    "print('Property shape:    ', lh_thickness.shape)\n",
    "print('Mesh vertex count: ', sub.lh.vertex_count)\n",
    "\n",
    "# Make a histogram of the values:\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(6,4), dpi=72*4)\n",
    "ax.hist(lh_thickness)\n",
    "ax.set_xlabel('Cortical Thickness [mm]')\n",
    "ax.set_ylabel('Frequency [vertices]')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can replot the 3D plot above using the cortical thickness as the overlay\n",
    "# for the surface; this time we will use the white surface instead of the pial:\n",
    "\n",
    "surf = 'white'\n",
    "view = 'posterior'\n",
    "\n",
    "# Note: the string 'thickness' can alternately be a vector of numbers--i.e.,\n",
    "# the thickness values for each vertex rather than the name of the property\n",
    "ny.cortex_plot((sub.lh.surfaces[surf], sub.rh.surfaces[surf]),\n",
    "               view=view, color='thickness', cmap='hot')\n",
    "\n",
    "# tell ipyvolume to show the plot:\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 3: Importing and Plotting the BOLD fMRI Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HCP performed retinotopic mapping experiments on 181 subjects. The data from these maps are available via the HCP's S3 interface, but only in unprocessed volumetric form. Rather than preprocess the volumes, we will use pre-processed data from the HCP. These preprocessed 7T functional data-files can be downloaded from [the Connectome Database](https://db.humanconnectome.org/).\n",
    "\n",
    "Note that these preprocessed files end with the suffic `.nii`. Although this is the file suffix for a nifti file, these are **not** actually nifti files. Rather, they are 'cifti' files. CIFTI is one of the HCP's custom file formats; part of the reason that cifti files are used rather than nifti files is because often the HCP needs to store data related to the cortical surface. For reasons related to the nifti file format, this is difficult to do with a nifti file. For the purpose of this tutorial, we can consider cifti files to be nifti files that store data on the cortical surface as well as in select subcortical voxels (like voxels in the LGN or the hippocampus).\n",
    "\n",
    "Typically, cifti files store data for HCP 'grayordinates'. A 'grayordinate' is a name made up by the HCP for either a cortical surface position (vertex) or a subcortical position (voxel). The cortical surface vertices are usually not the subject's native surface vertices but one of the atlas surface vertices. The atlases are called `fs_LR` surfaces and are named by the resolution of the vertices; for the LH, the atlases are: `lh_LR32k` (~32k vertices per hemisphere), `lh_LR59k` (~59k vertices per hemisphere), and `lh_LR164k` (~164k vertices per hemisphere). For this demo, we will use the 32k-resolution files. To begin, we will just plot the 32k resolution hemisphere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemi = 'LR59k'\n",
    "surf = 'white'\n",
    "\n",
    "# Make a basic plot of both hemispheres of this atlas surface:\n",
    "ny.cortex_plot((sub.hemis['lh_'+hemi].surfaces[surf],\n",
    "                sub.hemis['rh_'+hemi].surfaces[surf]))\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The BOLD Data\n",
    "\n",
    "The BOLD time-series data is stored in a set of cifti files; we can import these with neuropythy (or nibabel). This cell may produce a warning from nibabel; it can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiments consist of the stimulus (rings, wedges, or bars) moving in various\n",
    "# directions across the screen; we will concatenate these files into one longer\n",
    "# stimulus file:\n",
    "exp_directions = ['EXP','CON','CW','CCW','BAR1','BAR2']\n",
    "enc_dir = {'BAR1':'AP', 'BAR2':'PA', 'CCW': 'AP', 'CW':  'PA', 'EXP': 'AP', 'CON': 'PA'}\n",
    "bold_images = {}\n",
    "for exp_dir in exp_directions:\n",
    "    filename = 'tfMRI_RET%s_7T_%s_Atlas_1.6mm_MSMAll.dtseries.nii' % (exp_dir, enc_dir[exp_dir])\n",
    "    filename = os.path.join('/data/hcp/retinotopy', str(sid), filename)\n",
    "    bold_images[exp_dir] = ny.load(filename, 'cifti', to='image')\n",
    "# now concatenate these images into a single image\n",
    "bold_image = np.asarray([np.asarray(bold_images[k].dataobj) for k in exp_directions])\n",
    "# subtract the mean as a simple form of detrending between stimulus sets:\n",
    "bold_image = bold_image - np.reshape(np.mean(bold_image, axis=1), (len(bold_image), 1, -1))\n",
    "bold_image = np.vstack(bold_image)\n",
    "bold_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the CIFTI file into LH/RH/Subcortical matrices\n",
    "(lh_bold, rh_bold, subctx_bold) = ny.hcp.cifti_split(bold_image)\n",
    "# What are the dimensions of these? (time x vertices/voxels)\n",
    "[lh_bold.shape, rh_bold.shape, subctx_bold.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an arbitrary surface vertex in the Brodmann 17 region; this should\n",
    "# be in V1; note that in the HCP datafiles, Brodmann 17 is labeled 10:\n",
    "(v1_idcs,) = np.where(sub.hemis['lh_'+hemi].prop('brodmann_area') == 10)\n",
    "ii = v1_idcs[12]\n",
    "signal = lh_bold[:,ii]\n",
    "\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(7,2.5), dpi=2*72)\n",
    "ax.plot(np.arange(len(signal)), signal, 'k.-', lw=0.25, ms=0.5, marker='.')\n",
    "\n",
    "ax.set_xlim([0,1800])\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('BOLD Signal [AU]')\n",
    "for k in ['top','right']: ax.spines[k].set_visible(False)\n",
    "# Adjust the x-axis to be nicer for glancing at time:\n",
    "ax.set_xticks(np.arange(0,1801,300))\n",
    "ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, let's plot the LH standard deviation of the BOLD time-course on the cortical surface. We'll use the inflated surface, which is better for viewing inside sulci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_bold = np.std(lh_bold, axis=0)\n",
    "mesh = sub.hemis['lh_'+hemi].surfaces['inflated']\n",
    "ny.cortex_plot(mesh, color=std_bold, cmap='hot')\n",
    "# Also, put a sphere on our mesh at the vertex we are examining:\n",
    "(x,y,z) = mesh.coordinates[:, ii]\n",
    "ipv.scatter([x,x], [y,y], [z,z], color='blue', marker='sphere')\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 4: Load and Plot the Stimulus Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to interpret the BOLD time-series data from the retinotopy experiment, we will also need to know what the stimulus looked like, approximately. The following cell loads the stimulus and rearranges it to be (rows x cols x frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stimulus:\n",
    "stimulus_arrays = {}\n",
    "for exp_dir in exp_directions:\n",
    "    ed = 'BAR' if exp_dir.startswith('BAR') else exp_dir\n",
    "    with h5py.File('/data/hcp/retinotopy/apertures/RET%ssmall.mat' % ed, 'r') as fl:\n",
    "        stimulus_arrays[exp_dir] = np.asarray(fl['stim'])\n",
    "for (k,v) in six.iteritems(stimulus_arrays):\n",
    "    # We swap rows/cols here because matlab stores things in column-major order\n",
    "    stimulus_arrays[k] = np.transpose(v / 255.0, [2,1,0])\n",
    "# make one big stimulus array:\n",
    "stimulus_array = np.concatenate([stimulus_arrays[k] for k in exp_directions], axis=2)\n",
    "# we also want to downsample it to just 80x80 for the sake of speed:\n",
    "from skimage.transform import resize as skimresize\n",
    "stimulus_array = skimresize(stimulus_array, [80,80,stimulus_array.shape[-1]])\n",
    "stimulus_array = np.round(stimulus_array).astype('short') # convert in short integer\n",
    "stimulus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total frames in the stimulus\n",
    "frames = stimulus_array.shape[-1]\n",
    "\n",
    "# Setup plot for select frames of the stimulus:\n",
    "(fig,axs) = plt.subplots(6,10, figsize=(14,9), dpi=72*2)\n",
    "axs = axs.flatten()\n",
    "n = len(axs) # number of frames we'll plot\n",
    "\n",
    "frame_nos = np.round(np.linspace(0, frames-1, n)).astype('int')\n",
    "for (fno,ax) in zip(frame_nos, axs):\n",
    "    ax.imshow(stimulus_array[:,:,fno], vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Frame %03d' % (fno,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Receptive Field Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we use popeye's \"Ordinary Gaussian\" model with a fitted HRF to solve for the pRF parameters. Popeye does this by performing a \"ballpark\" grid search then taking the best grid-search result and performing a non-linear minimization from this starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import popeye.og_hrf as og\n",
    "from popeye.visual_stimulus import VisualStimulus\n",
    "\n",
    "# Wrap the stimulus array up in Popeye's stimulus interpreter; it needs a bit of\n",
    "# additional information to, for example, convert from pixels to degrees:\n",
    "screen_dist         = 100   # distance to screen...\n",
    "screen_width        = 28.11 # with screen_dist=100, this gives a 16° aperture\n",
    "ballpark_downsample = 0.5   # downsample stimulus frames by this much for quick estimates\n",
    "tr_length           = 1.0   # scan time (seconds) between BOLD images in the bold signal\n",
    "\n",
    "stimulus = VisualStimulus(stimulus_array, screen_dist, screen_width,\n",
    "                          ballpark_downsample, tr_length, 'short')\n",
    "hrf   = popeye.utilities.double_gamma_hrf\n",
    "model = og.GaussianModel(stimulus, hrf)\n",
    "model.hrf_delay = -0.25 # this must be set manually for _hrf models\n",
    "\n",
    "# popeye uses a sparse grid-fit followed by a fine-detail nonlinear fit; for both of these\n",
    "# we want to specify boundaries for the searches. These are (min,max) parameter values for\n",
    "# the parameters x0, y0, sigma, and hrf_delay (in that order).\n",
    "# For the grid-fit, we use slightly less extreme limits so that we don't start at extreme\n",
    "# positions in the nonlinear fit:\n",
    "ballpark_bounds = {'x':(-8, 8),  'y':(-8, 8),  'sigma':(0.20,6),  'hrf':(-2,2)}\n",
    "estimate_bounds = {'x':(-16,16), 'y':(-16,16), 'sigma':(0.01,12), 'hrf':(-7,7)}\n",
    "\n",
    "# The fitting process will take a few minutes\n",
    "param_order = ['x','y','sigma','hrf']\n",
    "fit = og.GaussianFit(model, signal,\n",
    "                     [ballpark_bounds[k] for k in param_order],\n",
    "                     [estimate_bounds[k] for k in param_order],\n",
    "                     Ns=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the found pRF parameters:\n",
    "text = r'''\n",
    "PRF Parameters:\n",
    "    (x0, y0): (%4.2f, %4.2f),\n",
    "       sigma: %4.2f\n",
    "        gain: %4.2f\n",
    "   HRF delay: %4.2f\n",
    "   r-squared: %4.2f\n",
    "''' % (fit.x, fit.y, fit.sigma, fit.beta, 5+fit.hrf_delay, fit.rsquared)\n",
    "\n",
    "# We'll now plot a visualization of the pRF itself...\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(7,7))\n",
    "\n",
    "# Sample the visual field at points from -10°, to 10°:\n",
    "max_eccen = 10\n",
    "res = 250\n",
    "samples = np.linspace(-max_eccen, max_eccen, res)\n",
    "(x,y) = np.meshgrid(samples, samples)\n",
    "# get the gaussian pRF sensitivity values:\n",
    "(x0,y0,sigma) = (fit.x, fit.y, fit.sigma)\n",
    "z = np.exp(-0.5*((x - x0)**2 + (y - y0)**2)/sigma**2)\n",
    "\n",
    "# Plot the z values:\n",
    "ax.pcolormesh(x, y, z, cmap='reddish', vmin=0, vmax=1, shading='gouraud')\n",
    "# some aesthetic things: plot things like ticks for the visual field:\n",
    "for sp in ax.spines.values(): sp.set_visible(False)\n",
    "tick_color = [0.75,0.75,0.75,1]\n",
    "for etick in [1,2,4,8]:\n",
    "    ax.add_patch(plt.Circle((0,0), etick, color=tick_color, lw=0.5, fill=False))\n",
    "ax.plot([-8,8],[0,0], color=tick_color, lw=0.5)\n",
    "ax.plot([0,0],[-8,8], color=tick_color, lw=0.5)\n",
    "# add the text to the axis\n",
    "ax.text(-8, 1, text, fontdict={'family':'monospace', 'horizontalalignment':'left'})\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted PRF response on top of the measured BOLD:\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(7,2.5), dpi=2*72)\n",
    "\n",
    "# plot the prediction in red on top of the data in black:\n",
    "signal = lh_bold[:,ii]\n",
    "ax.plot(np.arange(len(signal)), signal, 'k.-', lw=0.25, ms=0.5, marker='.')\n",
    "pred = fit.prediction\n",
    "ax.plot(np.arange(len(signal)), pred,   'r-', lw=1)\n",
    "\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_xlim([0,1800])\n",
    "ax.set_ylabel('BOLD Signal [AU]')\n",
    "for k in ['top','right']: ax.spines[k].set_visible(False)\n",
    "# Adjust the x-axis to be nicer for glancing at time:\n",
    "ax.set_xticks(np.arange(0,1801,300))\n",
    "ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retinotopic Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at the pRF solutions across the cortical surface: the retinotopic maps. The pRF data for all the subjects that participated in retinotopic mapping experiments in the HCP have already been solved and can be auto-downloaded by neuropythy. The OSF page for the HCP 7T retinotopy dataset project is [here](https://osf.io/bw9ec/), and the paper is [here](https://doi.org/10.1167/18.13.23).\n",
    "\n",
    "The pRF parameters are saved as properties of the hemisphere objects of the subjects. There are 'lowres-prf_' and 'highres-prf_' properties as well as just 'prf_' properties (e.g., 'lowres-prf_polar_angle', 'prf_sigma'). The 'highres-prf_' properties are the results from the higher-resolution (59k) interpolation of the retinotopy data; the 'lowres-prf_' properties are the results solved on the 32k meshes. As of the time of this course, the high resolution results are not yet available and can be ignored (neuropythy will raise an exception if you try to use them). In the future, these will be added to the OSF and can be used then. The 'prf_' parameters are the 'highres-' parameters if they are available and otherwise the 'lowres-prf_' parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pRF maps for the entire LH of the subject on the subject's native\n",
    "# white surface; we'll also put a dot at the closest point to the vertex we\n",
    "# previously solved\n",
    "\n",
    "# In the following lines, we instruct neuropythy to plot the inflated surface\n",
    "# (lh.white_surface is an alias for lh.surfaces['white']) and to color it\n",
    "# according to the prf_polar_angle property. We also add a mask for the\n",
    "# color overlay; a tuple (property, min, max) is one way to specify a mask;\n",
    "# the mask could also be a binary mask or a list of vertex indices, or\n",
    "# (property, (val1, val2, ...)).\n",
    "# When neuropythy is given a color name that ends with polar_angle,\n",
    "# eccentricity, or a variety of other recognized names, it will automatically\n",
    "# pick a color schema; however, you can use the cmap and related matplotlib\n",
    "# arguments here as well.\n",
    "surf = 'inflated'\n",
    "mesh = sub.lh.surfaces[surf]\n",
    "fig = ny.cortex_plot(mesh,\n",
    "                     color='prf_polar_angle',\n",
    "                     mask=('prf_variance_explained', 0.04, np.inf))\n",
    "\n",
    "# Find the closest point to the vertex we picked out earlier; to do this we\n",
    "# use a registration, which is a sphereically inflated surface. Registrations\n",
    "# of a hemisphere to a particular named space ('fsaverage', 'fs_LR') can be\n",
    "# considered roughly aligned as far as vertex position between hemispheres\n",
    "# is concerned.\n",
    "lh_LR = sub.hemis['lh_'+hemi]\n",
    "pt = lh_LR.registrations['fs_LR'].coordinates[:,ii]\n",
    "lh_reg_LR = sub.lh.registrations['fs_LR']\n",
    "# find the nearest vertex in the shared fs_LR registration to pt;\n",
    "# note that <hemisphere>.vertex_hash is a scipy spatial hash of the vertices\n",
    "nearest = lh_reg_LR.vertex_hash.query(pt, 1)[1]\n",
    "# Plot a blue sphere at this position:\n",
    "(x,y,z) = mesh.coordinates[:,nearest]\n",
    "ipv.scatter([x,x], [y,y], [z,z], color='blue', marker='sphere')\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to our solution\n",
    "\n",
    "We can check how close our particular fit is to the fit found by analyzePRF (the HCP pRF results auto-loaded by neuropythy were solved using [analyzePRF](https://kendrickkay.net/analyzePRF/) by Kendrick Kay). To do this, we will pull the property off the nearest vertex we found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters bundled in neuropythy for the nearest vertex:\n",
    "fit = {k: sub.lh.prop('prf_'+v)[nearest]\n",
    "       for (k,v) in zip(['x', 'y', 'sigma',  'beta', 'rsquared'],\n",
    "                        ['x', 'y', 'radius', 'gain', 'variance_explained'])}\n",
    "fit = ny.util.data_struct(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reproduce the pRF plot from above using these new parameters\n",
    "text = r'''\n",
    "PRF Parameters:\n",
    "    (x0, y0): (%4.2f, %4.2f),\n",
    "       sigma: %4.2f\n",
    "        gain: %4.2f\n",
    "   r-squared: %4.2f\n",
    "''' % (fit.x, fit.y, fit.sigma, fit.beta, fit.rsquared)\n",
    "\n",
    "# We'll now plot a visualization of the pRF itself...\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(7,7))\n",
    "\n",
    "# Sample the visual field at points from -10°, to 10°:\n",
    "max_eccen = 10\n",
    "res = 250\n",
    "samples = np.linspace(-max_eccen, max_eccen, res)\n",
    "(x,y) = np.meshgrid(samples, samples)\n",
    "# get the gaussian pRF sensitivity values:\n",
    "(x0,y0,sigma) = (fit.x, fit.y, fit.sigma)\n",
    "z = np.exp(-0.5*((x - x0)**2 + (y - y0)**2)/sigma**2)\n",
    "\n",
    "# Plot the z values:\n",
    "ax.pcolormesh(x, y, z, cmap='reddish', vmin=0, vmax=1, shading='gouraud')\n",
    "# some aesthetic things: plot things like ticks for the visual field:\n",
    "for sp in ax.spines.values(): sp.set_visible(False)\n",
    "tick_color = [0.75,0.75,0.75,1]\n",
    "for etick in [1,2,4,8]:\n",
    "    ax.add_patch(plt.Circle((0,0), etick, color=tick_color, lw=0.5, fill=False))\n",
    "ax.plot([-8,8],[0,0], color=tick_color, lw=0.5)\n",
    "ax.plot([0,0],[-8,8], color=tick_color, lw=0.5)\n",
    "# add the text to the axis\n",
    "ax.text(-8, 1, text, fontdict={'family':'monospace', 'horizontalalignment':'left'})\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 7: Predict Retinotopy and/or Apply an Atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Benson 2014 Atlas\n",
    "\n",
    "In these cells, we apply the Benson 2014 atlas to a subject; this is a prediction, based entirely on the subject's anatomy, of the retinotopic maps in V1-V3 (as well as a number of other maps, but note that only the V1, V2, and V3 maps should be considered reliable--see [this paper](https://doi.org/10.7554/eLife.40224) for more information). The Benson 2014 maps include both V1, V2, and V3 ROI boundaries as well as polar angle and eccentricity predictions for every vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the prediction for both hemispheres:\n",
    "benson14_rmaps = {h: ny.vision.predict_retinotopy(sub.hemis[h])\n",
    "                  for h in ['lh','rh']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the polar angle prediction in V1-V3 for the LH:\n",
    "ny.cortex_plot(sub.lh.surfaces['inflated'],\n",
    "               color=benson14_rmaps['lh']['angle'],\n",
    "               mask=(benson14_rmaps['lh']['varea'], (1,2,3)),\n",
    "               # Since we didn't give a property name, we have to tell\n",
    "               # neuropythy the colormap and min/max\n",
    "               cmap='polar_angle', vmin=-180, vmax=180)\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Wang 2015 Atlas\n",
    "\n",
    "The Wang et al. (2015) probabilistic atlas of visual areas is a handy atlas for determining approximately where in the visual hierarchy a position on cortex is. It is based on the average boundary lines as drawn by hand across 53 subjects. See [this publication](https://doi.org/10.1093/cercor/bhu277) for additional details. Unlike the Benson 2014 maps, the Wang 2015 atlas provides information about visual area boundaries but does not include information about the organization of the retinotopic maps themselves.\n",
    "\n",
    "Because the Wang atlas is defined on the fsaverage surface, we can interpolate from the fsaverage spherical surface onto a subject's fsaverage-registered spherical surface. Both FreeSurfer and HCP subjects' native hemispheres are usually aligned to the fsaverage hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load the wang atlas; there is a copy bundled with neuropythy:\n",
    "atlas_path = os.path.join(ny.util.library_path(), 'data', 'fsaverage', 'surf')\n",
    "wang15_atlas = {h: ny.load(os.path.join(atlas_path, '%s.wang15_mplbl.v1_0.mgz' % h))\n",
    "                for h in ['lh','rh']}\n",
    "# The wang atlas is just a property (of area labels) for each vertex on the\n",
    "# fsaverage hemisphere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, interpolate from fsaverage over to our subject:\n",
    "fsa = ny.freesurfer_subject('fsaverage')\n",
    "# because the data are integers, this is automatically interpolated using nearest\n",
    "# neighbor (though you can override with the method option); alignments to the\n",
    "# fsaverage are detected automatically and used for surface interpolation\n",
    "lh_wang15 = fsa.lh.interpolate(sub.lh, wang15_atlas['lh'])\n",
    "\n",
    "# okay, now make a plot of the want atlas; we can make a custom label/annotation\n",
    "# colormap using a utility function:\n",
    "cm = ny.graphics.label_cmap(lh_wang15)\n",
    "ny.cortex_plot(sub.lh.surfaces['inflated'], color=lh_wang15, cmap=cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retinotopic Maps and Model-based Analysis of fMRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Noah C. Benson &lt;[nben@nyu.edu](mailto:nben@nyu.edu)&gt;  \n",
    "**Repository**: [https://github.com/noahbenson/neurohackademy2019](https://github.com/noahbenson/neurohackademy2019)\n",
    "\n",
    "This notebook is an introduction to retinotopic maps and model-based analysis of fMRI, created for the Neurohackademy 2019 summer course by Noah C. Benson. In this notebook, we will use Python and in particular the [neuropythy](https://github.com/noahbenson/neuropythy) and [popeye](https://github.com/kdesimone/popeye) open source libraries to download both anatomical/structural data and functional data (BOLD time-series) from the [Human Commectome Project](https://db.humanconnectome.org/) then to analyze the time-series. This notebook is intended to be a verbose description of the various tools and techniques involved in such an analysis; it is likely that we will not cover all the material in the notebook during the Neurohackademy lecture.\n",
    "\n",
    "The contents of this notebook are as follows:\n",
    "\n",
    "* **Introduction**  \n",
    "  This section gives a brief introduction to the project in the notebook.\n",
    "* **Initialization**  \n",
    "  In this section of the notebook, we import libraries, configure the plotting environment, and declare custom functions that we will need throughout the notebook. We will also verify that you have configured your environment correctly for this notebook to work.\n",
    "* **Demo 1**: Plot Structural data from the HCP.\n",
    "  Here, we will load the relevant anatomical data from the HCP's Amazon S3 interface using [neuropythy](https://github.com/noahbenson/neuropythy) then examine and verify those data by plotting them.\n",
    "* **Demo 2**: Solve a pRF model and plot the results.  \n",
    "  This section documents population receptive field (pRF) models and demonstrates how to fit a pRF model from the fMRI data obtained in the previous section. We will use pRF data from the HCP that has been preprocessed and prepared for this tutorial.\n",
    "* **Demo 3**: Plot retinotopic maps.\n",
    "  Finally, we examine retinotopic maps--the organization of pRFs across the cortical surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import a number of libraries and configure matplotlib/pyplot, our 2D plotting tool. We then check that neuropythy is able to communicate with the Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Configure Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some standard/utility libraries:\n",
    "import os, sys, time, h5py, zipfile\n",
    "import six           # six provides python 2/3 compatibility\n",
    "\n",
    "# Import our numerical/scientific libraries, scipy and numpy:\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# The pimms (Python Immutables) library is a utility library that enables lazy\n",
    "# computation and immutble data structures; https://github.com/noahbenson/pimms\n",
    "import pimms\n",
    "\n",
    "# The popeye library (https://github.com/kdesimone/popeye) is a pRF modeling\n",
    "# library that we will use to obtain pRF parameters for the functional data\n",
    "import popeye\n",
    "\n",
    "# The neuropythy library is a swiss-army-knife for handling MRI data, especially\n",
    "# anatomical/structural data such as that produced by FreeSurfer or the HCP.\n",
    "# https://github.com/noahbenson/neuropythy\n",
    "import neuropythy as ny\n",
    "\n",
    "# Import graphics libraries:\n",
    "# Matplotlib/Pyplot is our 2D graphing library:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# We also use the 3D graphics library ipyvolume for 3D surface rendering\n",
    "import ipyvolume as ipv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These \"magic commands\" tell matplotlib that we want to plot figures inline and\n",
    "# That we are using qt as a backend; due to bugs in certain versions of\n",
    "# matplotlib, we put them in a separate cell as the import statements above and\n",
    "# the configuration statements below.\n",
    "%gui qt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional matplotlib preferences:\n",
    "font_data = {'family':'sans-serif',\n",
    "             'sans-serif':['Helvetica Neue', 'Helvetica', 'Arial'],\n",
    "             'size': 10,\n",
    "             'weight': 'light'}\n",
    "mpl.rc('font',**font_data)\n",
    "# we want relatively high-res images, especially when saving to disk.\n",
    "mpl.rcParams['figure.dpi'] = 72*2\n",
    "mpl.rcParams['savefig.dpi'] = 72*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check neuropythy's Amazon S3 Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [neuropythy](https://github.com/noahbenson/neuropythy) library can easily be configured to automatically download HCP data it is requested. In order to do this, however, it must have been given a set of HCP credentials. The HCP uses the Amazon S3 so these credentials are in the form of a \"key\" and a \"secret\". To obtain HCP credentials, you must register at the [HCP database website](https://db.humanconnectome.org/) then generate Amazon S3 credentials through their interface. The [neuropythy configuration documentation](https://github.com/noahbenson/neuropythy/wiki/Configuration) explains how to do this in more detail.\n",
    "\n",
    "Your credentials will look something like \"`AKAIG8RT71SWARPYUFUS`\" and \"`TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4`\" (key and secret). They are often printed with a \"`:`\" between them like \"`AKAIG8RT71SWARPYUFUS:TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4`\". If, when you started up the docker-container running this notebook, you provided your credentials as a `:`-separated string like this via the environment variable HCP_CREDENTIALS, you should be fine. If neuropythy found your credentials, they should be in neuropythy's `config` structure, which behaves like a Python dictionary. However, if you are running this tutorial in the Neurohackademy 2019 class itself, you likely were not able to do this and will have to set these credentials manually. If neuropythy could not read your credentials or if they were not set, then `ny.config['hcp_credentials']` will be `None`. If this is the case, you can either:\n",
    " * set the credentials directly in this notebook by running something like:  \n",
    "   ```python\n",
    "   key = 'AKAIG8RT71SWARPYUFUS'\n",
    "   secret = 'TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4'\n",
    "   ny.config['hcp_credentials'] = (key, secret)\n",
    "   ```\n",
    " * restart the docker container after configuring the `HCP_CREDENTIALS` environment variable:\n",
    "   ```bash\n",
    "   > export HCP_CREDENTIALS=\"AKAIG8RT71SWARPYUFUS:TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4\"\n",
    "   > docker-compose up\n",
    "   ```\n",
    " * store the credentials in a local file and import its contents into the `HCP_CREDENTIALS` environment variable:\n",
    "   ```bash\n",
    "   > echo \"AKAIG8RT71SWARPYUFUS:TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4\" > ~/.hcp-passwd\n",
    "   > export HCP_CREDENTIALS=\"`cat ~/.hcp-passwd`\"\n",
    "   > docker-compose up\n",
    "   ```\n",
    "\n",
    "We also want to check that neuropythy was able to connect to the HCP database. If this fails, it is possible that either your credentials are incorrect/expired or that you do not have a valid internet connection, or that you did something unexpected when mounting volumes into the docker that prevent neuropythy from knowing where to store the HCP data (unlikely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that HCP credentials were found:\n",
    "if ny.config['hcp_credentials'] is None:\n",
    "    raise Exception('No valid HCP credentials were found!\\n'\n",
    "                    'See above instructions for remedy.')\n",
    "\n",
    "# Check that we can access the HCP database:\n",
    "# To do this we grab the 's3fs' object from neuropythy's 'hcp' dataset; this\n",
    "# object maintains a connection to Amazon's S3 using the hcp credentials. We use\n",
    "# it to perform a basic ls operation on the S3 filesystem. If this fails, we do\n",
    "# not have a working connection to the S3.\n",
    "try: files = ny.data['hcp'].s3fs.ls('hcp-openaccess')\n",
    "except Exception: files = None\n",
    "if files is None:\n",
    "    raise Exception('Could not communicate with S3!\\n'\n",
    "                    'This probably indicates that your credentials are wrong'\n",
    "                    ' or that you do not have an internet connection.')\n",
    "\n",
    "print('Configuration appears fine!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Load a Subject and Plot their Pial Surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the tutorial we will choose a subject from the HCP and plot structural data from that subject.\n",
    "\n",
    "In many projects, the task of wrangling data into a format that enables your analysis is 98% of the work, but becomes only 2% of the resulting scientific paper. When dealing with fMRI data, this difficulty often arises from the complex and often poorly-documented formats and directory structures used to store the data. Both FreeSurfer and the HCP Pipelines are capable of processing a T1-weighted anatomical image into a rich set of (mostly equivalent) information about the cortical surface of the brain, its anatomy, and how its geometry maps onto anatomical atlases (like FreeSurfer's *fsaverage* subject or the HCP's *fs_LR* hemispheres). However, interpreting these data is a chore, and, even if you understand the directories and formats, the amount of code required to load the relevant data for an analysis can be quite substantial.\n",
    "\n",
    "One of the main goals of neuropythy is to reduce the burden of both understanding and coding access to these data. For the human connectome project in particular, neuropythy will automatically download and manage structural data as if it were all always on your local storage without you needing to think about it. This section will demo these abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a sample subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the following demos, we need to choose an example HCP subject to use. I have chosen one below, but you should be able to use basically any HCP subject you like, so long as they are in the HCP_1200 release. To see a list of these subjects, you can ask neuropythy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 10 HCP_1200 subjects:\n",
    "ny.hcp.subject_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many HCP_1200 subjects are there?\n",
    "len(ny.hcp.subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to be doing analysis on the retinotopic maps, we will eventually want to choose a subject that was part of the retinotopic mapping experiments. To get a list of these subjects, we can ask neuropythy's 'hcp_retinotopy' dataset for its list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many retinotopy subjects?\n",
    "len(ny.data['hcp_retinotopy'].subject_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are actually only 181 HCP subjects with retinotopy, but the dataset includes three additional subjects: subject 999999 (group average of all 181 subjects) and subjects 999998 and 999997 (group average of split halves of the 181 subjects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the subject we will use for the following demos\n",
    "sid = 100610\n",
    "\n",
    "# Make sure that's a real subject\n",
    "if sid not in ny.hcp.subject_ids:\n",
    "    raise Exception('Subject %s is not an HCP_1200 subject!' % (sid,))\n",
    "elif sid not in ny.data['hcp_retinotopy'].subject_ids:\n",
    "    print('Subject %s is not a retinotopy subject!' % (sid,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some structural data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will load the HCP subject we picked above using neuropythy; neuropythy will give us a subject object that encapsulates a large amount of data about the subject, including volumetric MR images as well as surface-based data. We will focus mostly on the surface data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the subject object:\n",
    "sub = ny.hcp_subject(sid)\n",
    "\n",
    "# We want to plot both the LH and the RH pial surfaces; this code uses the 'lh'\n",
    "# and 'rh' hemispheres which are the subject's \"native\" hemispheres. The HCP\n",
    "# also includes a number of \"atlas\" hemispheres that are roughly equivalent but\n",
    "# that have been coerced to a common geometrical tesselation. These hemispheres\n",
    "# contain less accurate representations of the subject's structural data but\n",
    "# have the advantage that vertices on the surface are approximately aligned\n",
    "# between subjects. For a list of all supported hemispheres, including atlas\n",
    "# hemispheres, you can see sub.hemis, which is a dictionary of all the known\n",
    "# hemisphere objects. sub.lh and sub.rh are aliases for sub.hemis['lh'] and\n",
    "# sub.hemis['rh'].\n",
    "\n",
    "surf = 'pial'       # we're plotting the pial surface\n",
    "view = 'posterior'  # we want a view of the posterior of the brain\n",
    "\n",
    "# make the plot; because we are passing in 3D surfaces, it returns an \n",
    "# ipyvolume figure (which we will not actually use)\n",
    "ny.cortex_plot((sub.lh.surfaces[surf], sub.rh.surfaces[surf]), view=view)\n",
    "\n",
    "# tell ipyvolume to show the plot:\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Structural Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometric structural data, such as the meshes plotted in the section above, are only one kind of data that are included in FreeSurfer and HCP subject directories. Neuropythy is capable of loading most of these data, and automatically organizes them, when possible, into properties that are associated with the hemisphere objects and surface meshes. An example of this is the cortical thickness (in mm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cortical thickness is actually just a vector of values, one per vertex in\n",
    "# the hemisphere's mesh representation. Properties are stored in numpy arrays\n",
    "# and can be accessed easily:\n",
    "lh_thickness = sub.lh.prop('thickness')\n",
    "print('Property shape:    ', lh_thickness.shape)\n",
    "print('Mesh vertex count: ', sub.lh.vertex_count)\n",
    "\n",
    "# Make a histogram of the values:\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(6,4), dpi=72*4)\n",
    "ax.hist(lh_thickness)\n",
    "ax.set_xlabel('Cortical Thickness [mm]')\n",
    "ax.set_ylabel('Frequency [vertices]')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can replot the 3D plot above using the cortical thickness as the overlay\n",
    "# for the surface; this time we will use the white surface instead of the pial:\n",
    "\n",
    "surf = 'white'\n",
    "view = 'posterior'\n",
    "\n",
    "# Note: the string 'thickness' can alternately be a vector of numbers--i.e.,\n",
    "# the thickness values for each vertex rather than the name of the property\n",
    "ny.cortex_plot((sub.lh.surfaces[surf], sub.rh.surfaces[surf]),\n",
    "               view=view, color='thickness', cmap='hot')\n",
    "\n",
    "# tell ipyvolume to show the plot:\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Fit a pRF model to BOLD data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HCP performed retinotopic mapping experiments on 181 subjects. The data from these maps are available via the HCP's S3 interface, but only in unprocessed volumetric form. Rather than preprocess the volumes, we will use some preprocessed data from the HCP that has already been prepared and included in the repository for this tutorial. Preprocessed 7T functional data-files can be downloaded from [the Connectome Database](https://db.humanconnectome.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The BOLD Data\n",
    "\n",
    "The BOLD time-series data we will be using has been preprocessed and prepared for the course; these data can be downloaded from the https://db.humanconnectome.org/ database website as CIFTI files. Because these files can be very large, we will just load part of a time-series from a JSON file that is part of the tutorial GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_data = ny.load('data/example_BOLD.json')\n",
    "signal = bold_data['timeseries']\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(7,2.5), dpi=2*72)\n",
    "ax.plot(np.arange(len(signal)), signal, 'k.-', lw=0.5, ms=1, marker='.')\n",
    "\n",
    "ax.set_xlim([0,600])\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('BOLD Signal [AU]')\n",
    "for k in ['top','right']: ax.spines[k].set_visible(False)\n",
    "# Adjust the x-axis to be nicer for glancing at time:\n",
    "ax.set_xticks(np.arange(0,601,300))\n",
    "ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stimulus Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to interpret the BOLD time-series data from the retinotopy experiment, we will also need to know what the stimulus looked like, approximately. The following cell loads the stimulus and rearranges it to be (rows x cols x frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stimulus data comes from a MAT file (really an HDF5 file) that is bundled\n",
    "# with the github repository for this tutorial; this file can be found on the\n",
    "# OSF page for the HCP 7T Retinotopy Dataset (https://osf.io/bw9ec/) in the\n",
    "# apertures.zip file.\n",
    "with h5py.File('data/RETBARsmall.mat', 'r') as fl:\n",
    "    stimulus_array = np.asarray(fl['stim'])\n",
    "# We swap rows/cols here because matlab stores things in column-major order\n",
    "stimulus_array = np.transpose(stimulus_array / 255.0, [2,1,0])\n",
    "# Since we are looking at BOLD time-series data from 2 runs of this stimulus, we\n",
    "# need to duplicate the stimulus array along the time axis (the third axis)\n",
    "stimulus_array = np.concatenate([stimulus_array, stimulus_array], axis=2)\n",
    "stimulus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total frames in the stimulus\n",
    "frames = stimulus_array.shape[-1]\n",
    "\n",
    "# Setup plot for select frames of the stimulus:\n",
    "(fig,axs) = plt.subplots(6,10, figsize=(14,9), dpi=72*2)\n",
    "axs = axs.flatten()\n",
    "n = len(axs) # number of frames we'll plot\n",
    "\n",
    "frame_nos = np.round(np.linspace(0, frames-1, n)).astype('int')\n",
    "for (fno,ax) in zip(frame_nos, axs):\n",
    "    ax.imshow(stimulus_array[:,:,fno], vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Frame %03d' % (fno,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the pRF Model using popeye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we use popeye's \"Ordinary Gaussian\" model with a fitted HRF to solve for the pRF parameters. Popeye does this by performing a \"ballpark\" grid search then taking the best grid-search result and performing a non-linear minimization from this starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import popeye.og_hrf as og\n",
    "from popeye.visual_stimulus import VisualStimulus\n",
    "\n",
    "# Wrap the stimulus array up in Popeye's stimulus interpreter; it needs a bit of\n",
    "# additional information to, for example, convert from pixels to degrees:\n",
    "screen_dist         = 100   # distance to screen...\n",
    "screen_width        = 28.11 # with screen_dist=100, this gives a 16° aperture\n",
    "ballpark_downsample = 0.5   # downsample stimulus frames by this much for quick estimates\n",
    "tr_length           = 1.0   # scan time (seconds) between BOLD images in the bold signal\n",
    "\n",
    "stimulus = VisualStimulus(stimulus_array[:,:,-600:], screen_dist, screen_width,\n",
    "                          ballpark_downsample, tr_length, 'short')\n",
    "hrf   = popeye.utilities.double_gamma_hrf\n",
    "model = og.GaussianModel(stimulus, hrf)\n",
    "model.hrf_delay = -0.25 # this must be set manually for _hrf models\n",
    "\n",
    "# popeye uses a sparse grid-fit followed by a fine-detail nonlinear fit; for both of these\n",
    "# we want to specify boundaries for the searches. These are (min,max) parameter values for\n",
    "# the parameters x0, y0, sigma, and hrf_delay (in that order).\n",
    "# For the grid-fit, we use slightly less extreme limits so that we don't start at extreme\n",
    "# positions in the nonlinear fit:\n",
    "ballpark_bounds = {'x':(-8, 8),  'y':(-8, 8),  'sigma':(0.20,6),  'hrf':(-2,2)}\n",
    "estimate_bounds = {'x':(-16,16), 'y':(-16,16), 'sigma':(0.01,12), 'hrf':(-7,7)}\n",
    "\n",
    "# The fitting process will take a few minutes\n",
    "param_order = ['x','y','sigma','hrf']\n",
    "fit = og.GaussianFit(model, signal[-600:],\n",
    "                     [ballpark_bounds[k] for k in param_order],\n",
    "                     [estimate_bounds[k] for k in param_order],\n",
    "                     Ns=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the found pRF parameters:\n",
    "text = r'''\n",
    "PRF Parameters:\n",
    "    (x0, y0): (%4.2f, %4.2f),\n",
    "       sigma: %4.2f\n",
    "        gain: %4.2f\n",
    "   HRF delay: %4.2f\n",
    "   r-squared: %4.2f\n",
    "''' % (fit.x, fit.y, fit.sigma, fit.beta, 5+fit.hrf_delay, fit.rsquared)\n",
    "\n",
    "# We'll now plot a visualization of the pRF itself...\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(7,7))\n",
    "\n",
    "# Sample the visual field at points from -10°, to 10°:\n",
    "max_eccen = 10\n",
    "res = 250\n",
    "samples = np.linspace(-max_eccen, max_eccen, res)\n",
    "(x,y) = np.meshgrid(samples, samples)\n",
    "# get the gaussian pRF sensitivity values:\n",
    "(x0,y0,sigma) = (fit.x, fit.y, fit.sigma)\n",
    "z = np.exp(-0.5*((x - x0)**2 + (y - y0)**2)/sigma**2)\n",
    "\n",
    "# Plot the z values:\n",
    "ax.pcolormesh(x, y, z, cmap='reddish', vmin=0, vmax=1, shading='gouraud')\n",
    "# some aesthetic things: plot things like ticks for the visual field:\n",
    "for sp in ax.spines.values(): sp.set_visible(False)\n",
    "tick_color = [0.75,0.75,0.75,1]\n",
    "for etick in [1,2,4,8]:\n",
    "    ax.add_patch(plt.Circle((0,0), etick, color=tick_color, lw=0.5, fill=False))\n",
    "ax.plot([-8,8],[0,0], color=tick_color, lw=0.5)\n",
    "ax.plot([0,0],[-8,8], color=tick_color, lw=0.5)\n",
    "# add the text to the axis\n",
    "ax.text(-8, 1, text, fontdict={'family':'monospace', 'horizontalalignment':'left'})\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted PRF response on top of the measured BOLD:\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(7,2.5), dpi=2*72)\n",
    "\n",
    "# plot the prediction in red on top of the data in black:\n",
    "ax.plot(np.arange(len(signal)), signal, 'k.-', lw=0.25, ms=0.5, marker='.')\n",
    "pred = fit.prediction\n",
    "ax.plot(np.arange(len(signal)), pred,   'r-', lw=1)\n",
    "\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_xlim([0,600])\n",
    "ax.set_ylabel('BOLD Signal [AU]')\n",
    "for k in ['top','right']: ax.spines[k].set_visible(False)\n",
    "# Adjust the x-axis to be nicer for glancing at time:\n",
    "ax.set_xticks(np.arange(0,601,300))\n",
    "ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retinotopic Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at the pRF solutions across the cortical surface: the retinotopic maps. The pRF data for all the subjects that participated in retinotopic mapping experiments in the HCP have already been solved and can be auto-downloaded by neuropythy. The OSF page for the HCP 7T retinotopy dataset project is [here](https://osf.io/bw9ec/), and the paper is [here](https://doi.org/10.1167/18.13.23).\n",
    "\n",
    "The pRF parameters are saved as properties of the hemisphere objects of the subjects. There are 'lowres-prf_' and 'highres-prf_' properties as well as just 'prf_' properties (e.g., 'lowres-prf_polar_angle', 'prf_sigma'). The 'highres-prf_' properties are the results from the higher-resolution (59k) interpolation of the retinotopy data; the 'lowres-prf_' properties are the results solved on the 32k meshes. As of the time of this course, the high resolution results are not yet available and can be ignored (neuropythy will raise an exception if you try to use them). In the future, these will be added to the OSF and can be used then. The 'prf_' parameters are the 'highres-' parameters if they are available and otherwise the 'lowres-prf_' parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pRF maps for the entire LH of the subject on the subject's native\n",
    "# white surface; we'll also put a dot at the closest point to the vertex we\n",
    "# previously solved\n",
    "\n",
    "# In the following lines, we instruct neuropythy to plot the inflated surface\n",
    "# (lh.white_surface is an alias for lh.surfaces['white']) and to color it\n",
    "# according to the prf_polar_angle property. We also add a mask for the\n",
    "# color overlay; a tuple (property, min, max) is one way to specify a mask;\n",
    "# the mask could also be a binary mask or a list of vertex indices, or\n",
    "# (property, (val1, val2, ...)).\n",
    "# When neuropythy is given a color name that ends with polar_angle,\n",
    "# eccentricity, or a variety of other recognized names, it will automatically\n",
    "# pick a color schema; however, you can use the cmap and related matplotlib\n",
    "# arguments here as well.\n",
    "surf = 'inflated'\n",
    "mesh = sub.lh.surfaces[surf]\n",
    "fig = ny.cortex_plot(mesh,\n",
    "                     color='prf_polar_angle',\n",
    "                     mask=('prf_variance_explained', 0.04, np.inf))\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to our solution\n",
    "\n",
    "We can check how close our particular fit is to the fit found by analyzePRF (the HCP pRF results auto-loaded by neuropythy were solved using [analyzePRF](https://kendrickkay.net/analyzePRF/) by Kendrick Kay). To do this, we will pull the property off the nearest vertex we found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters bundled in neuropythy for the nearest vertex:\n",
    "fit = {k: sub.lh.prop('prf_'+v)[nearest]\n",
    "       for (k,v) in zip(['x', 'y', 'sigma',  'beta', 'rsquared'],\n",
    "                        ['x', 'y', 'radius', 'gain', 'variance_explained'])}\n",
    "fit = ny.util.data_struct(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reproduce the pRF plot from above using these new parameters\n",
    "text = r'''\n",
    "PRF Parameters:\n",
    "    (x0, y0): (%4.2f, %4.2f),\n",
    "       sigma: %4.2f\n",
    "        gain: %4.2f\n",
    "   r-squared: %4.2f\n",
    "''' % (fit.x, fit.y, fit.sigma, fit.beta, fit.rsquared)\n",
    "\n",
    "# We'll now plot a visualization of the pRF itself...\n",
    "(fig,ax) = plt.subplots(1,1, figsize=(7,7))\n",
    "\n",
    "# Sample the visual field at points from -10°, to 10°:\n",
    "max_eccen = 10\n",
    "res = 250\n",
    "samples = np.linspace(-max_eccen, max_eccen, res)\n",
    "(x,y) = np.meshgrid(samples, samples)\n",
    "# get the gaussian pRF sensitivity values:\n",
    "(x0,y0,sigma) = (fit.x, fit.y, fit.sigma)\n",
    "z = np.exp(-0.5*((x - x0)**2 + (y - y0)**2)/sigma**2)\n",
    "\n",
    "# Plot the z values:\n",
    "ax.pcolormesh(x, y, z, cmap='reddish', vmin=0, vmax=1, shading='gouraud')\n",
    "# some aesthetic things: plot things like ticks for the visual field:\n",
    "for sp in ax.spines.values(): sp.set_visible(False)\n",
    "tick_color = [0.75,0.75,0.75,1]\n",
    "for etick in [1,2,4,8]:\n",
    "    ax.add_patch(plt.Circle((0,0), etick, color=tick_color, lw=0.5, fill=False))\n",
    "ax.plot([-8,8],[0,0], color=tick_color, lw=0.5)\n",
    "ax.plot([0,0],[-8,8], color=tick_color, lw=0.5)\n",
    "# add the text to the axis\n",
    "ax.text(-8, 1, text, fontdict={'family':'monospace', 'horizontalalignment':'left'})\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 7: Predict Retinotopy and/or Apply an Atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Benson 2014 Atlas\n",
    "\n",
    "In these cells, we apply the Benson 2014 atlas to a subject; this is a prediction, based entirely on the subject's anatomy, of the retinotopic maps in V1-V3 (as well as a number of other maps, but note that only the V1, V2, and V3 maps should be considered reliable--see [this paper](https://doi.org/10.7554/eLife.40224) for more information). The Benson 2014 maps include both V1, V2, and V3 ROI boundaries as well as polar angle and eccentricity predictions for every vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the prediction for both hemispheres:\n",
    "benson14_rmaps = {h: ny.vision.predict_retinotopy(sub.hemis[h])\n",
    "                  for h in ['lh','rh']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the polar angle prediction in V1-V3 for the LH:\n",
    "ny.cortex_plot(sub.lh.surfaces['inflated'],\n",
    "               color=benson14_rmaps['lh']['angle'],\n",
    "               mask=(benson14_rmaps['lh']['varea'], (1,2,3)),\n",
    "               # Since we didn't give a property name, we have to tell\n",
    "               # neuropythy the colormap and min/max\n",
    "               cmap='polar_angle', vmin=-180, vmax=180)\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Wang 2015 Atlas\n",
    "\n",
    "The Wang et al. (2015) probabilistic atlas of visual areas is a handy atlas for determining approximately where in the visual hierarchy a position on cortex is. It is based on the average boundary lines as drawn by hand across 53 subjects. See [this publication](https://doi.org/10.1093/cercor/bhu277) for additional details. Unlike the Benson 2014 maps, the Wang 2015 atlas provides information about visual area boundaries but does not include information about the organization of the retinotopic maps themselves.\n",
    "\n",
    "Because the Wang atlas is defined on the fsaverage surface, we can interpolate from the fsaverage spherical surface onto a subject's fsaverage-registered spherical surface. Both FreeSurfer and HCP subjects' native hemispheres are usually aligned to the fsaverage hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load the wang atlas; there is a copy bundled with neuropythy:\n",
    "atlas_path = os.path.join(ny.util.library_path(), 'data', 'fsaverage', 'surf')\n",
    "wang15_atlas = {h: ny.load(os.path.join(atlas_path, '%s.wang15_mplbl.v1_0.mgz' % h))\n",
    "                for h in ['lh','rh']}\n",
    "# The wang atlas is just a property (of area labels) for each vertex on the\n",
    "# fsaverage hemisphere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, interpolate from fsaverage over to our subject:\n",
    "fsa = ny.freesurfer_subject('fsaverage')\n",
    "# because the data are integers, this is automatically interpolated using nearest\n",
    "# neighbor (though you can override with the method option); alignments to the\n",
    "# fsaverage are detected automatically and used for surface interpolation\n",
    "lh_wang15 = fsa.lh.interpolate(sub.lh, wang15_atlas['lh'])\n",
    "\n",
    "# okay, now make a plot of the want atlas; we can make a custom label/annotation\n",
    "# colormap using a utility function:\n",
    "cm = ny.graphics.label_cmap(lh_wang15)\n",
    "ny.cortex_plot(sub.lh.surfaces['inflated'], color=lh_wang15, cmap=cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
